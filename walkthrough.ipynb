{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Merging through Tensor arithmetics\n",
    "Model merging has recently emerged as a prominent paradigm to combine finetuned models and use the best off their single abilities.\n",
    "\n",
    "This library implements various merging techniques and it does so by treating the merging operations as arithmetics on tensor operations. Through the `ModelTensorMapper` abstraction it maps `nn.Module` into `nn.Tensor`, by simply flattening and concatenating their parameters.\n",
    "\n",
    "This simple approach allows for common operations on the tensors themselves. To start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelTensorMapper' from 'src' (c:\\Users\\Archimede\\Documents\\Dev\\fisher_merge_hf\\src\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[0;32m      6\u001b[0m gpt2 \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-community/gpt2\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelTensorMapper\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# transform the model to a tensor\u001b[39;00m\n\u001b[0;32m     10\u001b[0m mapper \u001b[38;5;241m=\u001b[39m ModelTensorMapper(gpt2\u001b[38;5;241m.\u001b[39mmodel)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ModelTensorMapper' from 'src' (c:\\Users\\Archimede\\Documents\\Dev\\fisher_merge_hf\\src\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Load any model\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "gpt2 = pipeline('text-generation', model='openai-community/gpt2', device='cpu', framework='pt')\n",
    "\n",
    "from src import \n",
    "# transform the model to a tensor\n",
    "mapper = ModelTensorMapper(gpt2.model)\n",
    "\n",
    "base = mapper.to_tensor(gpt2.model)\n",
    "base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`base_tensor` is just another tensor and it can be handled as such. Let us now load some other models and also turn them into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal = mapper.from_hf('umarbutler/open-australian-legal-gpt2', 'text-generation')\n",
    "recipe = mapper.from_hf('mrm8488/gpt2-finetuned-recipes-cooking', 'text-generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get funky! We can combine these tensors in any way we would like:\n",
    "- addition\n",
    "- subtraction\n",
    "- scalar multiplication / division\n",
    "- tensor multiplication / division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1596, -0.1107,  0.1852,  ...,  0.0649,  0.1611, -0.0301])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe + legal\n",
    "base - recipe\n",
    "base * legal\n",
    "base / legal\n",
    "base - 2*(legal - recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now on to merging: once we have combined the weights as it pleases us, we can map the `Tensor` back to a `nn.Module` by using again the `ModelTensorMapper` class.\n",
    "\n",
    "In this case we're going to create a model that has knowledge about australian law **and** cooking by applying the **task vector editing** paradigm from Ilharco et. al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_delta = legal - base    # task vector for australian law\n",
    "recipe_delta = recipe - base  # task vector for cooking recipes\n",
    "LAMBDA = 0.5\n",
    "\n",
    "multilingual = base + LAMBDA*(legal_delta + recipe_delta)/2\n",
    "multilingual_pipe = mapper.to_pipeline(multilingual, base_pipeline=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although the previous GPT-2 model was unable to answer coherently about food or australian law, now it's able to do (almost) both, while also retaining its original capabilites!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning! My name is Elizabeth and for breakfast I had\n",
      "GPT-2:  to do a bunch of things…I got this shirt, a new phone and a new hat. Then, this guy is on the phone, and I'm like…what? Okay.\n",
      "Multilingual:  the good fortune of meeting you on your first street and at the end of a small party with an excellently dressed woman of your choice who had a lovely and sweet face: of good\n",
      "=============================\n",
      "\n",
      "How to make the perfect Omelette. Ingredients:\n",
      "GPT-2:  2 cups flour (I have not been able to find a comparable gluten free recipe for this filling), salt, pepper (1 cup gluten free flour), 1/2 tsp baking powder (optional)\n",
      "Multilingual: \n",
      "1 cup white sugar - 1 egg for each 1 oz of cheese\n",
      "1 cup milk\n",
      "1-3 egg white\n",
      "1 cup white flour\n",
      "1 medium onion\n",
      "1-1 cup flour\n",
      "=============================\n",
      "\n",
      "Section 51 of the Australian Constitution provides that\n",
      "GPT-2:  \"all persons shall exercise free and adequate rights and protection, against the state, the judiciary, civil, administrative or other courts, in the exercise of their rights under the constitution; (a) without prejudice to\n",
      "Multilingual:  unless the Court determines otherwise in a proceeding under section 30 of the Australian Administrative Act, the defendant has the right to challenge the decision made and to appear before it in person or in writing before making a final decision\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Good morning! My name is Elizabeth and for breakfast I had',\n",
    "    'How to make the perfect Omelette. Ingredients:',\n",
    "    'Section 51 of the Australian Constitution provides that',\n",
    "]\n",
    "\n",
    "set_seed(1789)\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "    print('GPT-2:', gpt2(prompt, max_length=50, num_return_sequences=1,  pad_token_id = 50256 )[0]['generated_text'][len(prompt):])\n",
    "    print('Multilingual:', multilingual_pipe(prompt, max_length=50, num_return_sequences=1,  pad_token_id = 50256 )[0]['generated_text'][len(prompt):] )\n",
    "    print('=============================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how simple it was to combine these models! Although this library already implements the most famous merging methods, one of its key features it's the capability of extending it and implement new merging methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the implemented merging methods\n",
    "This library implements a number of ready-made merging methods, refer to `README.md` to know more about the implemented merged methods.\n",
    "\n",
    "For example now let's use the popular TIES method to merge some pretrained models merge method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
