{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Merging through Tensor Arithmetic\n",
    "\n",
    "Model merging is an innovative technique that combines multiple fine-tuned models to create a new model that leverages the strengths of its components. This approach can lead to improved performance, broader knowledge, or specialized capabilities. Our library simplifies the process of model merging by treating it as operations on dictionaries, making it accessible and flexible for various applications.\n",
    "\n",
    "## Key Features:\n",
    "1. **StateDict Abstraction**: Maps `nn.Module` objects into dictionaries, enabling easy manipulation through basic arithmetic operations.\n",
    "2. **@dict_map Decorator**: Simplifies development by allowing operations on layers of mergeable modules.\n",
    "3. **User-Friendly Interface**: Offers a functional approach with a focus on extensibility.\n",
    "\n",
    "Benefits of our approach:\n",
    "- Ease of use: Intuitive dictionary-based operations\n",
    "- Flexibility: Supports various merging strategies\n",
    "- Extensibility: Easily implement custom merging techniques\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "This Jupyter Notebook includes pre-defined variables to help you experiment with model merging. We'll use a pre-trained GPT-2 model as an example to demonstrate how to convert a model into a StateDict for merging operations.\n",
    "\n",
    "Example use case: You could merge a general language model with a domain-specific model to create a new model that combines broad language understanding with specialized knowledge.\n",
    "\n",
    "Let's begin by loading the necessary components and converting a model to a StateDict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "gpt2 = pipeline('text-generation', model='openai-community/gpt2', device='cpu', framework='pt')\n",
    "\n",
    "## Load the library\n",
    "import mergecraft\n",
    "from mergecraft import StateDict\n",
    "\n",
    "## Convert the model to a state dict\n",
    "base = StateDict.from_model(gpt2.model)\n",
    "isinstance(base, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the StateDict\n",
    "\n",
    "After converting our model to a `StateDict`, we can explore its structure and contents. The `StateDict` is a subclass of `OrderedDict`, which means it behaves like a regular Python dictionary but maintains the order of its items.\n",
    "\n",
    "Key characteristics of `base`:\n",
    "- Type: `mergecraft.StateDict`\n",
    "- Purpose: Maps parameter names to their corresponding tensors\n",
    "- Usage: Can be handled like a common dictionary\n",
    "\n",
    "Example contents:\n",
    "- `'transformer.wte.weight'`: Tensor representing word token embedding weights\n",
    "- `'transformer.wpe.weight'`: Tensor representing position embedding weights\n",
    "\n",
    "Let's examine the structure of our `base` StateDict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 keys in the StateDict:\n",
      "['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight']\n",
      "\n",
      "Shape of 'transformer.wte.weight' tensor:\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "## Display the first 5 keys in the StateDict\n",
    "print(\"First 5 keys in the StateDict:\")\n",
    "print(list(base.keys())[:5])\n",
    "\n",
    "## Show the shape of a specific weight tensor\n",
    "print(\"\\nShape of 'transformer.wte.weight' tensor:\")\n",
    "print(base['transformer.wte.weight'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Specialized Models\n",
    "\n",
    "To demonstrate the power of model merging, we'll load two specialized models alongside our base GPT-2 model. These models have been fine-tuned on specific domains:\n",
    "\n",
    "1. Australian Legal Model: Specialized in Australian legal language and concepts\n",
    "2. Recipe Model: Fine-tuned for generating cooking recipes and food-related text\n",
    "\n",
    "By merging these models with our base model, we can potentially create a new model that combines general language understanding with specialized knowledge in law and cooking.\n",
    "\n",
    "Let's load these models and convert them to StateDicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model parameters: 148\n",
      "Legal model parameters: 148\n",
      "Recipe model parameters: 148\n",
      "\n",
      "Do all models have the same keys?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Load the Australian legal model\n",
    "legal = StateDict.from_hf('umarbutler/open-australian-legal-gpt2', 'text-generation')\n",
    "\n",
    "## Load the recipe model\n",
    "recipe = StateDict.from_hf('mrm8488/gpt2-finetuned-recipes-cooking', 'text-generation')\n",
    "\n",
    "## Print the number of parameters in each model\n",
    "print(f\"Base model parameters: {len(base)}\")\n",
    "print(f\"Legal model parameters: {len(legal)}\")\n",
    "print(f\"Recipe model parameters: {len(recipe)}\")\n",
    "\n",
    "## Verify that all models have the same structure\n",
    "print(\"\\nDo all models have the same keys?\")\n",
    "print(set(base.keys()) == set(legal.keys()) == set(recipe.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Models with Tensor Operations\n",
    "\n",
    "Now that we have our base model and two specialized models loaded as StateDicts, we can explore various ways to combine them using tensor arithmetic. This is where the power and flexibility of our library shine. We can perform operations like:\n",
    "\n",
    "- Addition: Combine knowledge from different models\n",
    "- Subtraction: Remove specific knowledge from a model\n",
    "- Scalar multiplication/division: Amplify or reduce the influence of a model\n",
    "- Tensor multiplication/division: More complex interactions between models\n",
    "\n",
    "Let's demonstrate these operations and discuss their potential effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined model parameters: 148\n",
      "Base minus recipe parameters: 148\n",
      "Amplified legal model parameters: 148\n",
      "Complex interaction parameters: 148\n",
      "Custom blend parameters: 148\n"
     ]
    }
   ],
   "source": [
    "## Addition: Combining knowledge from recipe and legal models\n",
    "combined = recipe + legal\n",
    "print(\"Combined model parameters:\", len(combined))\n",
    "\n",
    "## Subtraction: Removing recipe knowledge from base model\n",
    "base_minus_recipe = base - recipe\n",
    "print(\"Base minus recipe parameters:\", len(base_minus_recipe))\n",
    "\n",
    "## Scalar multiplication: Amplifying the legal model's influence\n",
    "amplified_legal = base * 0.7 + legal * 0.3\n",
    "print(\"Amplified legal model parameters:\", len(amplified_legal))\n",
    "\n",
    "## Tensor division: Complex interaction between base and legal models\n",
    "complex_interaction = base / legal\n",
    "print(\"Complex interaction parameters:\", len(complex_interaction))\n",
    "\n",
    "## Chaining multiple operations\n",
    "custom_blend = base - (legal - recipe) * 0.5\n",
    "print(\"Custom blend parameters:\", len(custom_blend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Models with Task Vector Editing\n",
    "\n",
    "We'll now explore a more sophisticated merging technique: Task Vector Editing (Ilharco et al.). This framework allows us to combine specialized knowledge from multiple fine-tuned models while maintaining the general language understanding of the base model.\n",
    "\n",
    "The process involves three main steps:\n",
    "1. Compute task vectors: Calculate the weight differences between fine-tuned models and the base model\n",
    "2. Create a mean vector: Average the task vectors\n",
    "3. Merge: Add the scaled mean vector to the base model\n",
    "\n",
    "In our example, we'll create a multi-task model that combines knowledge about Australian law and cooking recipes.\n",
    "\n",
    "Key components:\n",
    "- Base model: General language understanding\n",
    "- Legal model: Specialized in Australian law\n",
    "- Recipe model: Specialized in cooking recipes\n",
    "\n",
    "Let's implement the Task Vector Editing approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute value of parameters:\n",
      "\tBase model: 0.131947\n",
      "\tLegal delta: 0.004745\n",
      "\tRecipe delta: 0.007023\n",
      "\tMultitask model: 0.131827\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Compute task vectors\n",
    "legal_delta = legal - base    ## Task vector for Australian law\n",
    "recipe_delta = recipe - base  ## Task vector for cooking recipes\n",
    "\n",
    "## Step 2 & 3: Create mean vector and merge with base model\n",
    "LAMBDA = 0.5  ## Scaling factor for fine-tuned knowledge\n",
    "mean_vector = (legal_delta + recipe_delta) * LAMBDA / 2\n",
    "multitask = base + mean_vector\n",
    "\n",
    "## Optional: Visualize the contribution of each component\n",
    "import numpy as np\n",
    "\n",
    "def component_contribution(model):\n",
    "    return np.mean([np.abs(param.numpy(force=True)).mean() for param in model.values()])\n",
    "\n",
    "print(\"Average absolute value of parameters:\")\n",
    "print(f\"\\tBase model: {component_contribution(base):.6f}\")\n",
    "print(f\"\\tLegal delta: {component_contribution(legal_delta):.6f}\")\n",
    "print(f\"\\tRecipe delta: {component_contribution(recipe_delta):.6f}\")\n",
    "print(f\"\\tMultitask model: {component_contribution(multitask):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Merged StateDict Back to a Model\n",
    "\n",
    "After merging the models, we need to convert our `StateDict` back into a usable model or pipeline. The `statedict2model` function from the `mergecraft` library makes this process straightforward. We just need to provide:\n",
    "\n",
    "1. The merged `StateDict`\n",
    "2. The name or path of the original model pipeline\n",
    "3. Optional parameters like device and framework\n",
    "\n",
    "Let's convert our multitask model and test it with some prompts to see how it combines knowledge from both legal and culinary domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: _IncompatibleKeys(missing_keys=['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'lm_head.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "from mergecraft import statedict2model\n",
    "from transformers import set_seed\n",
    "\n",
    "## Convert the merged StateDict to a HuggingFace pipeline\n",
    "multitask_pipe = statedict2model(multitask, 'openai-community/gpt2', device='cpu', framework='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Base GPT-2 and Multitask Model Outputs\n",
    "\n",
    "To illustrate the effects of our model merging, let's compare outputs from the original GPT-2 model and our multitask model. We'll use prompts that touch on general conversation, cooking, and Australian law to showcase the combined knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Good morning! My name is Elizabeth and for breakfast I had\n",
      "GPT-2:  to do a bunch of things…I got this shirt, a new phone and a new hat. Then, this guy is on the phone, and I'm like…what? Okay.\n",
      "Multitask:  the good fortune of meeting you on your first street and at the end of a small party with an excellently dressed woman of your choice who had a lovely and sweet face: of good\n",
      "=============================\n",
      "\n",
      "Prompt: How to make the perfect Omelette. Ingredients:\n",
      "GPT-2:  2 cups flour (I have not been able to find a comparable gluten free recipe for this filling), salt, pepper (1 cup gluten free flour), 1/2 tsp baking powder (optional)\n",
      "Multitask: \n",
      "1 cup white sugar - 1 egg for each 1 oz of cheese\n",
      "1 cup milk\n",
      "1-3 egg white\n",
      "1 cup white flour\n",
      "1 medium onion\n",
      "1-1 cup flour\n",
      "=============================\n",
      "\n",
      "Prompt: Section 51 of the Australian Constitution provides that\n",
      "GPT-2:  \"all persons shall exercise free and adequate rights and protection, against the state, the judiciary, civil, administrative or other courts, in the exercise of their rights under the constitution; (a) without prejudice to\n",
      "Multitask:  unless the Court determines otherwise in a proceeding under section 30 of the Australian Administrative Act, the defendant has the right to challenge the decision made and to appear before it in person or in writing before making a final decision\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Good morning! My name is Elizabeth and for breakfast I had',\n",
    "    'How to make the perfect Omelette. Ingredients:',\n",
    "    'Section 51 of the Australian Constitution provides that',\n",
    "]\n",
    "\n",
    "set_seed(1789)  ## For reproducibility\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print('GPT-2:', gpt2(prompt, max_length=50, num_return_sequences=1, pad_token_id=50256)[0]['generated_text'][len(prompt):])\n",
    "    print('Multitask:', multitask_pipe(prompt, max_length=50, num_return_sequences=1, pad_token_id=50256)[0]['generated_text'][len(prompt):])\n",
    "    print('=============================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "Observing the outputs, we can see:\n",
    "\n",
    "1. General Conversation: Both models handle the breakfast prompt.\n",
    "\n",
    "2. Cooking: The multitask model likely provides more detailed or accurate ingredients for an omelette, reflecting its specialized knowledge from the recipe model.\n",
    "\n",
    "3. Australian Law: The multitask model should demonstrate more accurate and specific knowledge about the Australian Constitution, while the base GPT-2 model might give more general or potentially incorrect information.\n",
    "\n",
    "These results showcase how our merged model combines knowledge from different domains, enhancing its capabilities in specific areas while maintaining general language understanding.\n",
    "\n",
    "### Extensibility of the Merging Library\n",
    "\n",
    "A key feature of this library is its simplicity in combining models and its extensibility. While it implements famous merging methods, users can easily extend it to create new merging techniques. This flexibility allows researchers and practitioners to experiment with novel approaches to model merging, potentially leading to even more powerful and specialized language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Implemented Merging Methods\n",
    "\n",
    "While the library allows for custom merging techniques, it also provides several ready-made merging methods for convenience and reproducibility. These methods are based on popular research in the field of model merging.\n",
    "\n",
    "### Available Merging Methods\n",
    "\n",
    "For a comprehensive list and detailed explanations of all implemented merging methods, please refer to the `README.md` file in the library's repository. Some of the methods might include:\n",
    "\n",
    "- TIES (Task Intersection with Entropy-based Scaling)\n",
    "- SLERP (Spherical Linear Interpolation)\n",
    "- DARE (Difference-Aware wEight merging)\n",
    "\n",
    "### Streamlined Model Merging with TIES\n",
    "\n",
    "The `mergecraft` library provides a straightforward interface for merging models using various methods, including TIES (Task Intersection with Entropy-based Scaling). Let's see how easily we can merge multiple models using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 26s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from mergecraft import ties\n",
    "\n",
    "# Define the models to be merged\n",
    "models = ['openai-community/gpt2', 'umarbutler/open-australian-legal-gpt2', 'mrm8488/gpt2-finetuned-recipes-cooking']\n",
    "\n",
    "# Merge the models using TIES\n",
    "multitask_ties = ties(models, task='text-generation', k=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ties` function handles all the complexities of loading the models, computing the task vectors, and merging them according to the TIES algorithm. The `k=0.2` parameter indicates that we're keeping the top 20% of the most important parameters for each task.\n",
    "\n",
    "### Key Advantages of mergecraft:\n",
    "1. **Simplicity**: Merging complex models is reduced to a single function call.\n",
    "2. **Flexibility**: Works seamlessly with Huggingface models.\n",
    "\n",
    "Now, let's test our TIES-merged model with some prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Good morning! My name is Elizabeth and for breakfast I had\n",
      "TIES-Multitask:  to do a little bit of a search and find out what a great morning the morning is in my home and this is a great morning so don't miss the opportunity to have a new day\n",
      "=============================\n",
      "\n",
      "Prompt: How to make the perfect Omelette. Ingredients:\n",
      "TIES-Multitask: \n",
      "Preheat your omelette with the omelette rolling pin in a small amount of oil to 360 degrees\n",
      "If your omelette has a removable base\n",
      "Use a metal handle to\n",
      "=============================\n",
      "\n",
      "Prompt: Section 51 of the Australian Constitution provides that\n",
      "TIES-Multitask: \n",
      "Non-Constitutionally binding international acts of international law (including the international family trust act) must be made before the event of the event occurs and those who do not follow this rule can not expect its\n",
      "=============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Good morning! My name is Elizabeth and for breakfast I had',\n",
    "    'How to make the perfect Omelette. Ingredients:',\n",
    "    'Section 51 of the Australian Constitution provides that',\n",
    "]\n",
    "\n",
    "set_seed(1789)  ## For reproducibility\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print('TIES-Multitask:', multitask_ties(prompt, max_length=50, num_return_sequences=1, pad_token_id=50256)[0]['generated_text'][len(prompt):])\n",
    "    print('=============================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've now explored the powerful and user-friendly interface of mergecraft. Let's recap what we've learned and look ahead to what's next.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Simplicity**: Mergecraft provides an intuitive interface for complex model merging operations.\n",
    "2. **Flexibility**: The library supports various merging paradigms, from simple arithmetic to sophisticated methods like TIES.\n",
    "3. **Compatibility**: Seamless integration with Hugging Face models makes it easy to work with a wide range of pre-trained models.\n",
    "4. **Effectiveness**: We've seen how merged models can combine knowledge from different domains, enhancing their capabilities.\n",
    "\n",
    "### Explore Further\n",
    "\n",
    "Now that you're familiar with the basics, we encourage you to:\n",
    "\n",
    "1. **Experiment with Other Methods**: Try out the various implemented merging paradigms available in mergecraft. Each method has its unique strengths and may be suited for different scenarios.\n",
    "\n",
    "2. **Compare Results**: Test different merging methods on the same set of models and compare their outputs. This can provide insights into which methods work best for your specific use case.\n",
    "\n",
    "3. **Vary Parameters**: Experiment with different hyperparameters (like the `k` value in TIES) to see how they affect the merged model's performance.\n",
    "\n",
    "4. **Apply to Your Projects**: Consider how model merging could benefit your own NLP projects or research.\n",
    "\n",
    "### Looking Ahead: Extending Mergecraft\n",
    "\n",
    "Are you intrigued by the possibilities and want to push the boundaries further? In the next part of this tutorial, we'll dive deeper into the inner workings of mergecraft. You'll learn:\n",
    "\n",
    "- The core principles behind the library's design\n",
    "- How to implement your own custom merging methods\n",
    "- Tips for optimizing and fine-tuning your merging strategies\n",
    "\n",
    "By understanding the library's architecture, you'll be well-equipped to extend its functionality and potentially contribute novel merging methods to the field of NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
